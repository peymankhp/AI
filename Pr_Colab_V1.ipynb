{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNunp+CiYGFWzvpANUUEuDZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peymankhp/AI/blob/main/Pr_Colab_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Professional Gold Price Prediction System - Google Colab Version\"\"\"\n",
        "# System imports\n",
        "import pandas as pd  # Data manipulation and analysis\n",
        "import numpy as np   # Numerical computations\n",
        "from sklearn.preprocessing import MinMaxScaler  # Data normalization\n",
        "from sklearn.model_selection import TimeSeriesSplit  # Time-series validation\n",
        "import logging  # For logging\n",
        "import os  # For file operations\n",
        "from datetime import datetime  # For timestamps\n",
        "import tensorflow as tf  # Explicitly import tensorflow\n",
        "import matplotlib.pyplot as plt  # For plotting\n",
        "import seaborn as sns  # For better plot styling\n",
        "import math\n",
        "from google.colab import drive  # For Google Drive integration\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"Google Drive mounted successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {str(e)}\")\n",
        "\n",
        "# Check for GPU availability\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"GPU is available and will be used\")\n",
        "    # Enable mixed precision training for better performance on GPU\n",
        "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "else:\n",
        "    print(\"No GPU found, using CPU\")\n",
        "\n",
        "# Set plot style\n",
        "plt.style.use('seaborn-v0_8')  # Use a built-in style\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set up base directory for Colab\n",
        "BASE_DIR = '/content/drive/MyDrive/Colab Notebooks'\n",
        "MODEL_DIR = os.path.join(BASE_DIR, 'models')\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Set up logging with more detailed format\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(os.path.join(BASE_DIR, 'gold_price_prediction.log')),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Configure TensorFlow logging\n",
        "tf.get_logger().setLevel(logging.ERROR)  # Reduce TensorFlow logging verbosity\n",
        "\n",
        "# Data file paths for Google Drive\n",
        "DATA_FILES = {\n",
        "    'train': os.path.join(BASE_DIR, 'Gold_Price_TA_training.xlsx'),\n",
        "    'validation': os.path.join(BASE_DIR, 'Gold_Price_TA_validation.xlsx'),\n",
        "    'test': os.path.join(BASE_DIR, 'Gold_Price_TA_test.xlsx')\n",
        "}\n",
        "\n",
        "# Verify data files exist\n",
        "for name, path in DATA_FILES.items():\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Warning: {name} file not found at {path}\")\n",
        "    else:\n",
        "        print(f\"Found {name} file at {path}\")\n",
        "\n",
        "# ==============================================\n",
        "# 1. DATA LOADING AND PREPROCESSING\n",
        "# ==============================================\n",
        "\n",
        "def resample_to_weekly(df):\n",
        "    \"\"\"\n",
        "    Resample daily data to weekly frequency\n",
        "    Args:\n",
        "        df: Daily DataFrame with DateTimeIndex\n",
        "    Returns:\n",
        "        Weekly resampled DataFrame\n",
        "    \"\"\"\n",
        "    # Resample using last observation of the week for all columns\n",
        "    weekly_df = df.resample('W').last()\n",
        "    logging.info(f\"Resampled data from {len(df)} daily to {len(weekly_df)} weekly observations\")\n",
        "    return weekly_df\n",
        "\n",
        "def load_and_preprocess_data(file_path, columns_to_keep=None):\n",
        "    \"\"\"\n",
        "    Comprehensive data loading and cleaning pipeline with improved error handling\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(f\"Loading data from {file_path}\")\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"The file {file_path} was not found\")\n",
        "\n",
        "        # Read Excel file\n",
        "        df = pd.read_excel(\n",
        "            file_path,\n",
        "            engine='openpyxl',\n",
        "            dtype={'Date': str}\n",
        "        )\n",
        "\n",
        "        if df.empty:\n",
        "            raise ValueError(\"The loaded file is empty\")\n",
        "\n",
        "        # Convert date column\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Keep only specified columns if provided\n",
        "        if columns_to_keep is not None:\n",
        "            columns_with_date = ['Date'] + columns_to_keep\n",
        "            df = df[columns_with_date]\n",
        "\n",
        "        # Set Date as index and sort\n",
        "        df = df.sort_values('Date').set_index('Date')\n",
        "\n",
        "        # Convert numeric columns with improved error handling\n",
        "        for col in df.select_dtypes(include=[object]).columns:\n",
        "            try:\n",
        "                # First try direct conversion\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            except Exception:\n",
        "                # If failed, try cleaning the data first\n",
        "                df[col] = df[col].str.replace(',', '.').str.replace(' ', '')\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "            # Log number of NaN values after conversion\n",
        "            nan_count = df[col].isna().sum()\n",
        "            if nan_count > 0:\n",
        "                logging.warning(f\"Column {col}: {nan_count} values converted to NaN\")\n",
        "\n",
        "        # Handle missing values\n",
        "        df = df.interpolate(method='time', limit_direction='both', limit=5)\n",
        "        df = df.fillna(method='ffill', limit=5)\n",
        "        df = df.fillna(method='bfill', limit=5)\n",
        "        df = df.fillna(df.mean())\n",
        "\n",
        "        # Create weekly version\n",
        "        weekly_df = resample_to_weekly(df)\n",
        "\n",
        "        logging.info(f\"Data shapes - Daily: {df.shape}, Weekly: {weekly_df.shape}\")\n",
        "        return df, weekly_df\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def calculate_prediction_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate various accuracy metrics for predictions\n",
        "    Args:\n",
        "        y_true: True values\n",
        "        y_pred: Predicted values\n",
        "    Returns:\n",
        "        Dictionary of accuracy metrics\n",
        "    \"\"\"\n",
        "    # Calculate percentage errors\n",
        "    percentage_errors = np.abs((y_true - y_pred) / y_true) * 100\n",
        "\n",
        "    # Calculate directional accuracy\n",
        "    direction_true = np.diff(y_true) > 0\n",
        "    direction_pred = np.diff(y_pred) > 0\n",
        "    directional_accuracy = np.mean(direction_true == direction_pred) * 100\n",
        "\n",
        "    metrics = {\n",
        "        'mape': np.mean(percentage_errors),  # Mean Absolute Percentage Error\n",
        "        'median_pe': np.median(percentage_errors),  # Median Percentage Error\n",
        "        'rmse': np.sqrt(np.mean((y_true - y_pred) ** 2)),  # Root Mean Square Error\n",
        "        'mae': np.mean(np.abs(y_true - y_pred)),  # Mean Absolute Error\n",
        "        'r2': 1 - (np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2)),  # R-squared\n",
        "        'directional_accuracy': directional_accuracy  # Directional Accuracy\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def get_common_columns(dataframes, missing_threshold=0.3):\n",
        "    \"\"\"\n",
        "    Identify common columns across all dataframes with acceptable missing value ratios\n",
        "    Args:\n",
        "        dataframes: List of pandas DataFrames to analyze\n",
        "        missing_threshold: Maximum acceptable ratio of missing values (default: 0.3 or 30%)\n",
        "    Returns:\n",
        "        List of column names that meet the criteria\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get set of columns from first dataframe\n",
        "        common_cols = set(dataframes[0].columns)\n",
        "\n",
        "        # Find intersection with all other dataframes\n",
        "        for df in dataframes[1:]:\n",
        "            common_cols = common_cols.intersection(df.columns)\n",
        "\n",
        "        # Convert to list and sort\n",
        "        common_cols = sorted(list(common_cols))\n",
        "        logging.info(f\"Found {len(common_cols)} columns common to all datasets\")\n",
        "\n",
        "        # Check missing value ratios\n",
        "        valid_columns = []\n",
        "        for col in common_cols:\n",
        "            # Skip Date column as it's handled separately\n",
        "            if col == 'Date':\n",
        "                continue\n",
        "\n",
        "            # Check missing ratio in all dataframes\n",
        "            max_missing_ratio = max(\n",
        "                df[col].isnull().mean()\n",
        "                for df in dataframes\n",
        "            )\n",
        "\n",
        "            if max_missing_ratio <= missing_threshold:\n",
        "                valid_columns.append(col)\n",
        "            else:\n",
        "                logging.warning(f\"Column {col} excluded due to high missing ratio: {max_missing_ratio:.2%}\")\n",
        "\n",
        "        logging.info(f\"Retained {len(valid_columns)} columns after missing value filtering\")\n",
        "        return valid_columns\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in get_common_columns: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# First load raw data to identify common columns\n",
        "raw_train_df = pd.read_excel(DATA_FILES['train'])\n",
        "raw_test_df = pd.read_excel(DATA_FILES['test'])\n",
        "\n",
        "# Get common columns that meet missing value threshold\n",
        "common_columns = get_common_columns([raw_train_df, raw_test_df])\n",
        "logging.info(f\"Identified {len(common_columns)} common columns across all datasets\")\n",
        "\n",
        "# Load and preprocess all datasets with common columns\n",
        "train_daily_df, train_weekly_df = load_and_preprocess_data(DATA_FILES['train'], common_columns)\n",
        "test_daily_df, test_weekly_df = load_and_preprocess_data(DATA_FILES['test'], common_columns)\n",
        "\n",
        "# ==============================================\n",
        "# 2. FEATURE ENGINEERING AND SCALING\n",
        "# ==============================================\n",
        "\n",
        "def scale_dataset(df, scaler, fit_scaler=False):\n",
        "    \"\"\"\n",
        "    Scale the dataset using the provided scaler\n",
        "    Args:\n",
        "        df: DataFrame to scale\n",
        "        scaler: sklearn scaler object\n",
        "        fit_scaler: Whether to fit the scaler on this data\n",
        "    Returns:\n",
        "        Scaled numpy array\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Convert DataFrame to numpy array\n",
        "        data = df.values\n",
        "\n",
        "        # Fit scaler if requested\n",
        "        if fit_scaler:\n",
        "            data_scaled = scaler.fit_transform(data)\n",
        "            logging.info(f\"Fitted and transformed data with shape: {data_scaled.shape}\")\n",
        "        else:\n",
        "            data_scaled = scaler.transform(data)\n",
        "            logging.info(f\"Transformed data with shape: {data_scaled.shape}\")\n",
        "\n",
        "        return data_scaled\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error scaling dataset: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Define target variable (gold closing price)\n",
        "target_column = 'Close'  # Value we want to predict\n",
        "\n",
        "# Create feature list (all columns except target)\n",
        "feature_columns = [col for col in common_columns if col != target_column]\n",
        "\n",
        "# Initialize normalization scalers\n",
        "daily_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "weekly_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "# Scale daily data\n",
        "train_daily_scaled = scale_dataset(train_daily_df, daily_scaler, fit_scaler=True)\n",
        "test_daily_scaled = scale_dataset(test_daily_df, daily_scaler)\n",
        "\n",
        "# Scale weekly data\n",
        "train_weekly_scaled = scale_dataset(train_weekly_df, weekly_scaler, fit_scaler=True)\n",
        "test_weekly_scaled = scale_dataset(test_weekly_df, weekly_scaler)\n",
        "\n",
        "# ==============================================\n",
        "# 3. LSTM TIME-SERIES MODEL ARCHITECTURE\n",
        "# ==============================================\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, BatchNormalization, Bidirectional, Concatenate, Add, LayerNormalization\n",
        "from tensorflow.keras.layers import MultiHeadAttention, GlobalAveragePooling1D\n",
        "from tensorflow.keras.losses import Huber, BinaryCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"\n",
        "    Custom learning rate schedule with warm-up and cosine decay\n",
        "    \"\"\"\n",
        "    initial_lr = 0.001\n",
        "    warmup_epochs = 5\n",
        "    decay_epochs = 95  # remaining epochs after warmup\n",
        "    min_lr = 0.00001\n",
        "\n",
        "    if epoch < warmup_epochs:\n",
        "        # Linear warmup\n",
        "        return initial_lr * (epoch + 1) / warmup_epochs\n",
        "    else:\n",
        "        # Cosine decay\n",
        "        progress = (epoch - warmup_epochs) / decay_epochs\n",
        "        cosine_decay = 0.5 * (1 + math.cos(math.pi * progress))\n",
        "        return max(min_lr, initial_lr * cosine_decay)\n",
        "\n",
        "def directional_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Custom metric to calculate directional accuracy\n",
        "    \"\"\"\n",
        "    # Calculate directions\n",
        "    dir_true = K.sign(y_true[1:] - y_true[:-1])\n",
        "    dir_pred = K.sign(y_pred[1:] - y_pred[:-1])\n",
        "\n",
        "    # Calculate accuracy\n",
        "    return K.mean(K.cast(K.equal(dir_true, dir_pred), 'float32'))\n",
        "\n",
        "def combined_loss(alpha=0.7):\n",
        "    \"\"\"\n",
        "    Combined loss function with both value and direction components\n",
        "    \"\"\"\n",
        "    value_loss = Huber(delta=1.0)\n",
        "    direction_loss = BinaryCrossentropy()\n",
        "\n",
        "    def loss(y_true, y_pred):\n",
        "        # Value component\n",
        "        v_loss = value_loss(y_true, y_pred)\n",
        "\n",
        "        # Directional component\n",
        "        dir_true = K.sign(y_true[1:] - y_true[:-1])\n",
        "        dir_pred = K.sign(y_pred[1:] - y_pred[:-1])\n",
        "        d_loss = direction_loss(\n",
        "            K.cast((dir_true + 1) / 2, 'float32'),\n",
        "            K.cast((dir_pred + 1) / 2, 'float32')\n",
        "        )\n",
        "\n",
        "        return alpha * v_loss + (1 - alpha) * d_loss\n",
        "\n",
        "    return loss\n",
        "\n",
        "def create_lstm_model(input_shape):\n",
        "    \"\"\"\n",
        "    Build enhanced LSTM neural network optimized for directional accuracy and Colab GPU\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(f\"Creating enhanced directional LSTM model with input shape: {input_shape}\")\n",
        "\n",
        "        # Enable mixed precision for better GPU performance\n",
        "        if tf.config.list_physical_devices('GPU'):\n",
        "            tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "        # Input layer\n",
        "        inputs = Input(shape=input_shape, name=\"Input\")\n",
        "\n",
        "        # First Bidirectional LSTM layer with GPU optimization\n",
        "        x = Bidirectional(LSTM(\n",
        "            units=256,\n",
        "            return_sequences=True,\n",
        "            kernel_regularizer='l2',\n",
        "            recurrent_regularizer='l2',\n",
        "            name=\"BiLSTM1\",\n",
        "            # Add CuDNN optimization for GPU\n",
        "            implementation=2\n",
        "        ))(inputs)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "\n",
        "        # Enhanced attention mechanism\n",
        "        attention_output = MultiHeadAttention(\n",
        "            num_heads=16,  # Increased heads for better pattern recognition\n",
        "            key_dim=32\n",
        "        )(x, x)\n",
        "        x = Add()([x, attention_output])\n",
        "        x = LayerNormalization()(x)\n",
        "\n",
        "        # Second Bidirectional LSTM layer\n",
        "        x = Bidirectional(LSTM(\n",
        "            units=128,\n",
        "            return_sequences=True,\n",
        "            kernel_regularizer='l2',\n",
        "            recurrent_regularizer='l2',\n",
        "            name=\"BiLSTM2\"\n",
        "        ))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "\n",
        "        # Third Bidirectional LSTM layer\n",
        "        x = Bidirectional(LSTM(\n",
        "            units=64,\n",
        "            return_sequences=False,\n",
        "            kernel_regularizer='l2',\n",
        "            recurrent_regularizer='l2',\n",
        "            name=\"BiLSTM3\"\n",
        "        ))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "\n",
        "        # Split into two paths: one for value, one for direction\n",
        "        # Value prediction path\n",
        "        value_dense = Dense(128, activation='selu')(x)\n",
        "        value_dense = BatchNormalization()(value_dense)\n",
        "        value_dense = Dropout(0.2)(value_dense)\n",
        "\n",
        "        value_dense = Dense(64, activation='selu')(value_dense)\n",
        "        value_dense = BatchNormalization()(value_dense)\n",
        "        value_dense = Dropout(0.2)(value_dense)\n",
        "\n",
        "        # Direction prediction path\n",
        "        dir_dense = Dense(128, activation='selu')(x)\n",
        "        dir_dense = BatchNormalization()(dir_dense)\n",
        "        dir_dense = Dropout(0.2)(dir_dense)\n",
        "\n",
        "        dir_dense = Dense(64, activation='selu')(dir_dense)\n",
        "        dir_dense = BatchNormalization()(dir_dense)\n",
        "        dir_dense = Dropout(0.2)(dir_dense)\n",
        "\n",
        "        # Combine paths\n",
        "        combined = Concatenate()([value_dense, dir_dense])\n",
        "\n",
        "        # Final processing\n",
        "        x = Dense(64, activation='selu')(combined)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "\n",
        "        x = Dense(32, activation='selu')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.1)(x)\n",
        "\n",
        "        # If using GPU, ensure final layer uses float32\n",
        "        if tf.config.list_physical_devices('GPU'):\n",
        "            outputs = Dense(1, dtype='float32', name=\"Output\")(x)\n",
        "        else:\n",
        "            outputs = Dense(1, name=\"Output\")(x)\n",
        "\n",
        "        # Create model\n",
        "        model = Model(inputs=inputs, outputs=outputs, name=\"DirectionalGoldPriceLSTM\")\n",
        "\n",
        "        # Compile with custom loss and metrics\n",
        "        initial_learning_rate = 0.001\n",
        "        optimizer = Adam(\n",
        "            learning_rate=initial_learning_rate,\n",
        "            clipnorm=1.0,\n",
        "            beta_1=0.9,\n",
        "            beta_2=0.999,\n",
        "            epsilon=1e-07\n",
        "        )\n",
        "\n",
        "        # Use mixed precision optimizer if GPU is available\n",
        "        if tf.config.list_physical_devices('GPU'):\n",
        "            optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=combined_loss(alpha=0.7),\n",
        "            metrics=['mae', 'mse', directional_accuracy]\n",
        "        )\n",
        "\n",
        "        logging.info(\"Enhanced directional LSTM model created successfully\")\n",
        "        model.summary(print_fn=logging.info)\n",
        "        return model\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error creating enhanced LSTM model: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# ==============================================\n",
        "# 4. RISK ANALYSIS COMPONENTS\n",
        "# ==============================================\n",
        "\n",
        "def calculate_value_at_risk(returns, confidence=0.95):\n",
        "    \"\"\"\n",
        "    Calculate maximum expected loss using historical simulation\n",
        "    Args:\n",
        "        returns: Array of historical returns\n",
        "        confidence: Probability threshold (95% default)\n",
        "    Returns:\n",
        "        VaR value (negative = potential loss)\n",
        "    \"\"\"\n",
        "    return np.percentile(returns, 100 * (1 - confidence))  # Nth percentile loss\n",
        "\n",
        "def monte_carlo_simulation(start_price, days, num_simulations, mu, sigma):\n",
        "    \"\"\"\n",
        "    Generate stochastic price paths using Geometric Brownian Motion\n",
        "    Args:\n",
        "        start_price: Current market price\n",
        "        days: Projection horizon\n",
        "        num_simulations: Number of scenarios\n",
        "        mu: Annualized return expectation\n",
        "        sigma: Annualized volatility\n",
        "    Returns:\n",
        "        Matrix of simulated price paths (days x simulations)\n",
        "    \"\"\"\n",
        "    # Calculate daily drift and volatility\n",
        "    daily_return = mu / days  # Daily expected return\n",
        "    daily_vol = sigma / np.sqrt(days)  # Daily volatility\n",
        "\n",
        "    # Generate random daily returns\n",
        "    shocks = np.random.normal(\n",
        "        daily_return,\n",
        "        daily_vol,\n",
        "        (days, num_simulations)  # 2D array of returns\n",
        "    )\n",
        "\n",
        "    # Convert to price paths using cumulative product\n",
        "    price_paths = start_price * np.cumprod(1 + shocks, axis=0)\n",
        "\n",
        "    return price_paths\n",
        "\n",
        "# ==============================================\n",
        "# 5. MODEL TRAINING FRAMEWORK\n",
        "# ==============================================\n",
        "\n",
        "def train_time_series_model(model, X_train, y_train, validation_data, epochs=100):\n",
        "    \"\"\"\n",
        "    Enhanced training process optimized for Colab environment\n",
        "    \"\"\"\n",
        "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    model_path = os.path.join(MODEL_DIR, f'gold_price_model_{timestamp}.h5')\n",
        "\n",
        "    # Add TensorBoard logging for Colab\n",
        "    log_dir = os.path.join(BASE_DIR, 'logs', timestamp)\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_directional_accuracy',\n",
        "            mode='max',\n",
        "            patience=25,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath=model_path,\n",
        "            monitor='val_directional_accuracy',\n",
        "            mode='max',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        LearningRateScheduler(lr_schedule),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=10,\n",
        "            min_lr=0.000001,\n",
        "            verbose=1\n",
        "        ),\n",
        "        TensorBoard(\n",
        "            log_dir=log_dir,\n",
        "            histogram_freq=1,\n",
        "            write_graph=True,\n",
        "            write_images=True,\n",
        "            update_freq='epoch',\n",
        "            profile_batch=0\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        logging.info(\"Starting enhanced directional training...\")\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=validation_data,\n",
        "            epochs=epochs,\n",
        "            batch_size=32,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        logging.info(f\"Model training completed. Best model saved to {model_path}\")\n",
        "        return history\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during model training: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def create_sequences(data, window_size):\n",
        "    \"\"\"\n",
        "    Create sequences for time series prediction\n",
        "    Args:\n",
        "        data: Scaled numpy array of shape (samples, features)\n",
        "        window_size: Number of time steps to look back\n",
        "    Returns:\n",
        "        X: Input sequences of shape (samples, window_size, features)\n",
        "        y: Target values of shape (samples, 1)\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - window_size):\n",
        "        X.append(data[i:(i + window_size)])\n",
        "        y.append(data[i + window_size, 0])  # Assuming first column is target\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plot training history showing loss and metrics over epochs\n",
        "    Args:\n",
        "        history: Keras history object from model training\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create figure with subplots\n",
        "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
        "\n",
        "        # Plot training and validation loss\n",
        "        ax1.plot(history.history['loss'], label='Training Loss')\n",
        "        ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        ax1.set_title('Model Loss Over Epochs')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Plot MAE\n",
        "        ax2.plot(history.history['mae'], label='Training MAE')\n",
        "        ax2.plot(history.history['val_mae'], label='Validation MAE')\n",
        "        ax2.set_title('Mean Absolute Error Over Epochs')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('MAE')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        # Plot MSE\n",
        "        ax3.plot(history.history['mse'], label='Training MSE')\n",
        "        ax3.plot(history.history['val_mse'], label='Validation MSE')\n",
        "        ax3.set_title('Mean Squared Error Over Epochs')\n",
        "        ax3.set_xlabel('Epoch')\n",
        "        ax3.set_ylabel('MSE')\n",
        "        ax3.legend()\n",
        "        ax3.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the plot\n",
        "        plot_path = os.path.join(BASE_DIR, 'training_history.png')\n",
        "        plt.savefig(plot_path)\n",
        "        plt.close()\n",
        "\n",
        "        logging.info(f\"Training history plot saved to {plot_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error plotting training history: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def plot_predictions(y_true, y_pred, title, save_path, dates=None):\n",
        "    \"\"\"\n",
        "    Plot actual vs predicted values\n",
        "    Args:\n",
        "        y_true: True values\n",
        "        y_pred: Predicted values\n",
        "        title: Plot title\n",
        "        save_path: Path to save the plot\n",
        "        dates: Optional array of dates for x-axis\n",
        "    \"\"\"\n",
        "    try:\n",
        "        plt.figure(figsize=(15, 8))\n",
        "\n",
        "        # Plot actual and predicted values\n",
        "        if dates is not None:\n",
        "            plt.plot(dates, y_true, label='Actual', alpha=0.8)\n",
        "            plt.plot(dates, y_pred, label='Predicted', alpha=0.8)\n",
        "            plt.xticks(rotation=45)\n",
        "        else:\n",
        "            plt.plot(y_true, label='Actual', alpha=0.8)\n",
        "            plt.plot(y_pred, label='Predicted', alpha=0.8)\n",
        "\n",
        "        plt.title(title)\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel('Gold Price (USD)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Add prediction accuracy metrics to the plot\n",
        "        metrics = calculate_prediction_accuracy(y_true, y_pred)\n",
        "        metrics_text = f\"MAPE: {metrics['mape']:.2f}%\\n\"\n",
        "        metrics_text += f\"RMSE: ${metrics['rmse']:.2f}\\n\"\n",
        "        metrics_text += f\"Directional Accuracy: {metrics['directional_accuracy']:.2f}%\"\n",
        "\n",
        "        plt.text(0.02, 0.98, metrics_text,\n",
        "                transform=plt.gca().transAxes,\n",
        "                verticalalignment='top',\n",
        "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the plot\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "\n",
        "        logging.info(f\"Prediction plot saved to {save_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error plotting predictions: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, scaler, freq='daily'):\n",
        "    \"\"\"\n",
        "    Evaluate model performance on test set\n",
        "    Args:\n",
        "        model: Trained Keras model\n",
        "        X_test: Test features\n",
        "        y_test: Test labels\n",
        "        scaler: Scaler used for the data\n",
        "        freq: Frequency of predictions ('daily' or 'weekly')\n",
        "    Returns:\n",
        "        Dictionary of evaluation metrics and predictions\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(f\"Evaluating {freq} model performance...\")\n",
        "        # Get model predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate metrics on scaled data\n",
        "        mse = np.mean((y_test - y_pred.flatten()) ** 2)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = np.mean(np.abs(y_test - y_pred.flatten()))\n",
        "\n",
        "        metrics = {\n",
        "            'mse': mse,\n",
        "            'rmse': rmse,\n",
        "            'mae': mae\n",
        "        }\n",
        "\n",
        "        # Inverse transform predictions and actual values\n",
        "        n_features = X_test.shape[2]  # Number of features\n",
        "        dummy_test = np.zeros((len(y_test), n_features))\n",
        "        dummy_test[:, 0] = y_test  # First column is the target (Close price)\n",
        "        y_test_actual = scaler.inverse_transform(dummy_test)[:, 0]\n",
        "\n",
        "        dummy_pred = np.zeros((len(y_pred), n_features))\n",
        "        dummy_pred[:, 0] = y_pred.flatten()\n",
        "        y_pred_actual = scaler.inverse_transform(dummy_pred)[:, 0]\n",
        "\n",
        "        # Calculate accuracy metrics\n",
        "        accuracy_metrics = calculate_prediction_accuracy(y_test_actual, y_pred_actual)\n",
        "        metrics.update(accuracy_metrics)\n",
        "\n",
        "        # Plot predictions\n",
        "        plot_path = os.path.join(BASE_DIR, f'predictions_{freq}.png')\n",
        "        plot_predictions(\n",
        "            y_test_actual,\n",
        "            y_pred_actual,\n",
        "            f'Gold Price Predictions vs Actual Values ({freq.capitalize()} Test Set)',\n",
        "            plot_path\n",
        "        )\n",
        "\n",
        "        logging.info(f\"{freq.capitalize()} model evaluation metrics:\")\n",
        "        for metric_name, metric_value in metrics.items():\n",
        "            if metric_name in ['mape', 'directional_accuracy']:\n",
        "                logging.info(f\"{metric_name}: {metric_value:.2f}%\")\n",
        "            else:\n",
        "                logging.info(f\"{metric_name}: {metric_value:.4f}\")\n",
        "\n",
        "        return metrics, y_pred_actual\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during model evaluation: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# ==============================================\n",
        "# 6. MAIN EXECUTION FLOW\n",
        "# ==============================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        logging.info(\"Starting Gold Price Prediction System on Google Colab\")\n",
        "\n",
        "        # Print environment information\n",
        "        logging.info(f\"TensorFlow version: {tf.__version__}\")\n",
        "        logging.info(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
        "\n",
        "        # Log data shapes for debugging\n",
        "        logging.info(f\"Training data shape: {train_daily_df.shape}, {train_weekly_df.shape}\")\n",
        "        logging.info(f\"Test data shape: {test_daily_df.shape}, {test_weekly_df.shape}\")\n",
        "\n",
        "        # Example risk analysis workflow\n",
        "        price_returns = train_daily_df['Close'].pct_change().dropna()  # Daily returns\n",
        "        var_95 = calculate_value_at_risk(price_returns)  # 95% confidence level\n",
        "        logging.info(f\"Daily 95% Value-at-Risk: {var_95*100:.2f}%\")\n",
        "\n",
        "        # Monte Carlo simulation example\n",
        "        logging.info(\"Starting Monte Carlo simulation...\")\n",
        "        mc_simulations = monte_carlo_simulation(\n",
        "            start_price=train_daily_df['Close'].iloc[-1],  # Latest price\n",
        "            days=252,  # Trading days in year\n",
        "            num_simulations=1000,  # Number of scenarios\n",
        "            mu=price_returns.mean() * 252,  # Annualized return\n",
        "            sigma=price_returns.std() * np.sqrt(252)  # Annualized volatility\n",
        "        )\n",
        "        logging.info(\"Monte Carlo simulation completed\")\n",
        "\n",
        "        # Create sequences for LSTM training\n",
        "        window_size = 60\n",
        "        logging.info(f\"Creating sequences with window size: {window_size}\")\n",
        "        X_train_daily, y_train_daily = create_sequences(train_daily_scaled, window_size)\n",
        "        X_train_weekly, y_train_weekly = create_sequences(train_weekly_scaled, window_size)\n",
        "        X_test_daily, y_test_daily = create_sequences(test_daily_scaled, window_size)\n",
        "        X_test_weekly, y_test_weekly = create_sequences(test_weekly_scaled, window_size)\n",
        "\n",
        "        # Log sequence shapes\n",
        "        logging.info(f\"Training sequences shape: X_daily={X_train_daily.shape}, y_daily={y_train_daily.shape}, X_weekly={X_train_weekly.shape}, y_weekly={y_train_weekly.shape}\")\n",
        "        logging.info(f\"Test sequences shape: X_daily={X_test_daily.shape}, y_daily={y_test_daily.shape}, X_weekly={X_test_weekly.shape}, y_weekly={y_test_weekly.shape}\")\n",
        "\n",
        "        # Create and train the LSTM model\n",
        "        input_shape_daily = (window_size, train_daily_scaled.shape[1])\n",
        "        input_shape_weekly = (window_size, train_weekly_scaled.shape[1])\n",
        "        lstm_model_daily = create_lstm_model(input_shape_daily)\n",
        "        lstm_model_weekly = create_lstm_model(input_shape_weekly)\n",
        "\n",
        "        # Train the LSTM model\n",
        "        history_daily = train_time_series_model(\n",
        "            lstm_model_daily,\n",
        "            X_train_daily,\n",
        "            y_train_daily,\n",
        "            validation_data=(X_test_daily, y_test_daily),\n",
        "            epochs=100\n",
        "        )\n",
        "        history_weekly = train_time_series_model(\n",
        "            lstm_model_weekly,\n",
        "            X_train_weekly,\n",
        "            y_train_weekly,\n",
        "            validation_data=(X_test_weekly, y_test_weekly),\n",
        "            epochs=100\n",
        "        )\n",
        "\n",
        "        # Plot training history\n",
        "        plot_training_history(history_daily)\n",
        "        plot_training_history(history_weekly)\n",
        "\n",
        "        # Evaluate model performance and get predictions\n",
        "        metrics_daily, y_pred_daily = evaluate_model(\n",
        "            lstm_model_daily,\n",
        "            X_test_daily,\n",
        "            y_test_daily,\n",
        "            daily_scaler,\n",
        "            freq='daily'\n",
        "        )\n",
        "        metrics_weekly, y_pred_weekly = evaluate_model(\n",
        "            lstm_model_weekly,\n",
        "            X_test_weekly,\n",
        "            y_test_weekly,\n",
        "            weekly_scaler,\n",
        "            freq='weekly'\n",
        "        )\n",
        "\n",
        "        # Compare daily vs weekly performance\n",
        "        logging.info(\"\\nModel Performance Comparison:\")\n",
        "        logging.info(\"============================\")\n",
        "        metrics_comparison = {\n",
        "            'Daily MAPE': f\"{metrics_daily['mape']:.2f}%\",\n",
        "            'Weekly MAPE': f\"{metrics_weekly['mape']:.2f}%\",\n",
        "            'Daily Directional Accuracy': f\"{metrics_daily['directional_accuracy']:.2f}%\",\n",
        "            'Weekly Directional Accuracy': f\"{metrics_weekly['directional_accuracy']:.2f}%\",\n",
        "            'Daily RMSE': f\"${metrics_daily['rmse']:.2f}\",\n",
        "            'Weekly RMSE': f\"${metrics_weekly['rmse']:.2f}\"\n",
        "        }\n",
        "        for metric, value in metrics_comparison.items():\n",
        "            logging.info(f\"{metric}: {value}\")\n",
        "\n",
        "        logging.info(\"Gold Price Prediction System completed successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred during execution: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def move_file_to_project_dir(filename, project_dir):\n",
        "    found_paths = find_files(filename)\n",
        "    if found_paths:\n",
        "        source_path = found_paths[0]  # Take the first found instance\n",
        "        dest_path = os.path.join(project_dir, filename)\n",
        "        try:\n",
        "            # Create project directory if it doesn't exist\n",
        "            os.makedirs(project_dir, exist_ok=True)\n",
        "            # Copy the file (using copy2 to preserve metadata)\n",
        "            shutil.copy2(source_path, dest_path)\n",
        "            print(f\"✅ Successfully moved {filename} to {project_dir}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error moving file: {str(e)}\")\n",
        "            return False\n",
        "    else:\n",
        "        print(f\"❌ Could not find {filename}\")\n",
        "        return False\n",
        "\n",
        "# Move both files if needed\n",
        "for filename in ['Gold_Price_TA_training.xlsx', 'Gold_Price_TA_test.xlsx']:\n",
        "    move_file_to_project_dir(filename, project_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E-5EnCrqEkbR",
        "outputId": "9ac82cd9-bc33-4d55-abb7-6456f57d52f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully\n",
            "No GPU found, using CPU\n",
            "Found train file at /content/drive/MyDrive/Colab Notebooks/Gold_Price_TA_training.xlsx\n",
            "Found validation file at /content/drive/MyDrive/Colab Notebooks/Gold_Price_TA_validation.xlsx\n",
            "Found test file at /content/drive/MyDrive/Colab Notebooks/Gold_Price_TA_test.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Column AGO(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column ALB(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column ARG(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column ARM(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column AUS(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column AUT(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column AUT(PPI).1: 3052 values converted to NaN\n",
            "WARNING:root:Column AZE(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column Agriculture: 3052 values converted to NaN\n",
            "WARNING:root:Column Aluminum: 3052 values converted to NaN\n",
            "WARNING:root:Column BEL(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column BGD(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column BGR(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column BIH(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column BLR(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column BRA(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column Banana, Europe: 3052 values converted to NaN\n",
            "WARNING:root:Column Banana, US: 3052 values converted to NaN\n",
            "WARNING:root:Column Barley: 3052 values converted to NaN\n",
            "WARNING:root:Column Base Metals: 3052 values converted to NaN\n",
            "WARNING:root:Column Beef **: 3052 values converted to NaN\n",
            "WARNING:root:Column Beverages: 3052 values converted to NaN\n",
            "WARNING:root:Column CAF(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column CAN(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column CHE(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column CHL(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column CHN(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column COL(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column CRI(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column CYP(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column CZE(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column Chicken **: 3052 values converted to NaN\n",
            "WARNING:root:Column Coal, Australian: 3052 values converted to NaN\n",
            "WARNING:root:Column Coal, South African **: 3052 values converted to NaN\n",
            "WARNING:root:Column Cocoa: 3052 values converted to NaN\n",
            "WARNING:root:Column Coconut oil: 3052 values converted to NaN\n",
            "WARNING:root:Column Coffee, Arabica: 3052 values converted to NaN\n",
            "WARNING:root:Column Coffee, Robusta: 3052 values converted to NaN\n",
            "WARNING:root:Column Copper: 3052 values converted to NaN\n",
            "WARNING:root:Column Cotton, A Index: 3052 values converted to NaN\n",
            "WARNING:root:Column Crude oil, Brent: 3052 values converted to NaN\n",
            "WARNING:root:Column Crude oil, Dubai: 3052 values converted to NaN\n",
            "WARNING:root:Column Crude oil, WTI: 3052 values converted to NaN\n",
            "WARNING:root:Column Crude oil, average: 3052 values converted to NaN\n",
            "WARNING:root:Column DAP: 3052 values converted to NaN\n",
            "WARNING:root:Column DEU(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column DNK(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column ECU(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column EGY(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column ESP(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column EST(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column Energy: 3052 values converted to NaN\n",
            "WARNING:root:Column FIN(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column FRA(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column Fertilizers: 3052 values converted to NaN\n",
            "WARNING:root:Column Fish meal: 3052 values converted to NaN\n",
            "WARNING:root:Column Food: 3052 values converted to NaN\n",
            "WARNING:root:Column GBR(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column GEO(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column GHA(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column GRC(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column Gold: 3052 values converted to NaN\n",
            "WARNING:root:Column Grains: 3052 values converted to NaN\n",
            "WARNING:root:Column Groundnut oil **: 3052 values converted to NaN\n",
            "WARNING:root:Column Groundnuts: 3052 values converted to NaN\n",
            "WARNING:root:Column HRV(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column HUN(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column IDN(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column IND(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column IRL(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column IRN(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column ISL(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column ISR(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column ITA(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column Iron ore, cfr spot: 3052 values converted to NaN\n",
            "WARNING:root:Column JOR(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column JPN(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column KAZ(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column KGZ(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column KOR(PPI): 3052 values converted to NaN\n",
            "WARNING:root:Column Lamb **: 3052 values converted to NaN\n",
            "WARNING:root:Column Lead: 3052 values converted to NaN\n",
            "WARNING:root:Column Liquefied natural gas, Japan: 3052 values converted to NaN\n",
            "WARNING:root:Column Logs, Cameroon: 3052 values converted to NaN\n",
            "WARNING:root:Column Logs, Malaysian: 3052 values converted to NaN\n",
            "WARNING:root:Column Maize: 3052 values converted to NaN\n",
            "WARNING:root:Column Metals  & Minerals: 3052 values converted to NaN\n",
            "WARNING:root:Column Natural gas index: 3052 values converted to NaN\n",
            "WARNING:root:Column Natural gas, Europe: 3052 values converted to NaN\n",
            "WARNING:root:Column Natural gas, US: 3052 values converted to NaN\n",
            "WARNING:root:Column Nickel: 3052 values converted to NaN\n",
            "WARNING:root:Column Non-energy: 3052 values converted to NaN\n",
            "WARNING:root:Column Oils & Meals: 3052 values converted to NaN\n",
            "WARNING:root:Column Orange: 3052 values converted to NaN\n",
            "WARNING:root:Column Other Food : 3052 values converted to NaN\n",
            "WARNING:root:Column Other Raw Mat: 3052 values converted to NaN\n",
            "WARNING:root:Column Palm kernel oil: 3052 values converted to NaN\n",
            "WARNING:root:Column Palm oil: 3052 values converted to NaN\n",
            "WARNING:root:Column Phosphate rock: 3052 values converted to NaN\n",
            "WARNING:root:Column Platinum: 3052 values converted to NaN\n",
            "WARNING:root:Column Plywood: 3052 values converted to NaN\n",
            "WARNING:root:Column Potassium chloride **: 3052 values converted to NaN\n",
            "WARNING:root:Column Precious Metals: 3052 values converted to NaN\n",
            "WARNING:root:Column Rapeseed oil: 3052 values converted to NaN\n",
            "WARNING:root:Column Raw Materials: 3052 values converted to NaN\n",
            "WARNING:root:Column Rice, Thai 25%: 3052 values converted to NaN\n",
            "WARNING:root:Column Rice, Thai 5%: 3052 values converted to NaN\n",
            "WARNING:root:Column Rice, Thai A.1: 3052 values converted to NaN\n",
            "WARNING:root:Column Rice, Viet Namese 5%: 3052 values converted to NaN\n",
            "WARNING:root:Column Rubber, RSS3: 3052 values converted to NaN\n",
            "WARNING:root:Column Rubber, TSR20 **: 3052 values converted to NaN\n",
            "WARNING:root:Column Sawnwood, Cameroon: 3052 values converted to NaN\n",
            "WARNING:root:Column Sawnwood, Malaysian: 3052 values converted to NaN\n",
            "WARNING:root:Column Shrimps, Mexican: 3052 values converted to NaN\n",
            "WARNING:root:Column Silver: 3052 values converted to NaN\n",
            "WARNING:root:Column Sorghum: 3052 values converted to NaN\n",
            "WARNING:root:Column Soybean meal: 3052 values converted to NaN\n",
            "WARNING:root:Column Soybean oil: 3052 values converted to NaN\n",
            "WARNING:root:Column Soybeans: 3052 values converted to NaN\n",
            "WARNING:root:Column Sugar, EU: 3052 values converted to NaN\n",
            "WARNING:root:Column Sugar, US: 3052 values converted to NaN\n",
            "WARNING:root:Column Sugar, world: 3052 values converted to NaN\n",
            "WARNING:root:Column Sunflower oil: 3052 values converted to NaN\n",
            "WARNING:root:Column TSP: 3052 values converted to NaN\n",
            "WARNING:root:Column Tea, Colombo: 3052 values converted to NaN\n",
            "WARNING:root:Column Tea, Kolkata: 3052 values converted to NaN\n",
            "WARNING:root:Column Tea, Mombasa: 3052 values converted to NaN\n",
            "WARNING:root:Column Tea, avg 3 auctions: 3052 values converted to NaN\n",
            "WARNING:root:Column Timber: 3052 values converted to NaN\n",
            "WARNING:root:Column Tin: 3052 values converted to NaN\n",
            "WARNING:root:Column Tobacco, US import u.v.: 3052 values converted to NaN\n",
            "WARNING:root:Column Total Index: 3052 values converted to NaN\n",
            "WARNING:root:Column Urea: 3052 values converted to NaN\n",
            "WARNING:root:Column Wheat, US HRW: 3052 values converted to NaN\n",
            "WARNING:root:Column Wheat, US SRW: 3052 values converted to NaN\n",
            "WARNING:root:Column Zinc: 3052 values converted to NaN\n",
            "<ipython-input-13-8b171748a444>:133: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df = df.fillna(method='ffill', limit=5)\n",
            "<ipython-input-13-8b171748a444>:134: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df = df.fillna(method='bfill', limit=5)\n",
            "WARNING:root:Column AGO(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column ALB(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column ARG(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column ARM(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column AUS(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column AUT(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column AUT(PPI).1: 735 values converted to NaN\n",
            "WARNING:root:Column AZE(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column Agriculture: 735 values converted to NaN\n",
            "WARNING:root:Column Aluminum: 735 values converted to NaN\n",
            "WARNING:root:Column BEL(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column BGD(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column BGR(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column BIH(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column BLR(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column BRA(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column Banana, Europe: 735 values converted to NaN\n",
            "WARNING:root:Column Banana, US: 735 values converted to NaN\n",
            "WARNING:root:Column Barley: 735 values converted to NaN\n",
            "WARNING:root:Column Base Metals: 735 values converted to NaN\n",
            "WARNING:root:Column Beef **: 735 values converted to NaN\n",
            "WARNING:root:Column Beverages: 735 values converted to NaN\n",
            "WARNING:root:Column CAF(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column CAN(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column CHE(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column CHL(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column CHN(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column COL(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column CRI(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column CYP(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column CZE(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column Chicken **: 735 values converted to NaN\n",
            "WARNING:root:Column Coal, Australian: 735 values converted to NaN\n",
            "WARNING:root:Column Coal, South African **: 735 values converted to NaN\n",
            "WARNING:root:Column Cocoa: 735 values converted to NaN\n",
            "WARNING:root:Column Coconut oil: 735 values converted to NaN\n",
            "WARNING:root:Column Coffee, Arabica: 735 values converted to NaN\n",
            "WARNING:root:Column Coffee, Robusta: 735 values converted to NaN\n",
            "WARNING:root:Column Copper: 735 values converted to NaN\n",
            "WARNING:root:Column Cotton, A Index: 735 values converted to NaN\n",
            "WARNING:root:Column Crude oil, Brent: 735 values converted to NaN\n",
            "WARNING:root:Column Crude oil, Dubai: 735 values converted to NaN\n",
            "WARNING:root:Column Crude oil, WTI: 735 values converted to NaN\n",
            "WARNING:root:Column Crude oil, average: 735 values converted to NaN\n",
            "WARNING:root:Column DAP: 735 values converted to NaN\n",
            "WARNING:root:Column DEU(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column DNK(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column ECU(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column EGY(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column ESP(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column EST(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column Energy: 735 values converted to NaN\n",
            "WARNING:root:Column FIN(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column FRA(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column Fertilizers: 735 values converted to NaN\n",
            "WARNING:root:Column Fish meal: 735 values converted to NaN\n",
            "WARNING:root:Column Food: 735 values converted to NaN\n",
            "WARNING:root:Column GBR(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column GEO(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column GHA(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column GRC(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column Gold: 735 values converted to NaN\n",
            "WARNING:root:Column Grains: 735 values converted to NaN\n",
            "WARNING:root:Column Groundnut oil **: 735 values converted to NaN\n",
            "WARNING:root:Column Groundnuts: 735 values converted to NaN\n",
            "WARNING:root:Column HRV(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column HUN(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column IDN(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column IND(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column IRL(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column IRN(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column ISL(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column ISR(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column ITA(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column Iron ore, cfr spot: 735 values converted to NaN\n",
            "WARNING:root:Column JOR(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column JPN(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column KAZ(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column KGZ(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column KOR(PPI): 735 values converted to NaN\n",
            "WARNING:root:Column Lamb **: 735 values converted to NaN\n",
            "WARNING:root:Column Lead: 735 values converted to NaN\n",
            "WARNING:root:Column Liquefied natural gas, Japan: 735 values converted to NaN\n",
            "WARNING:root:Column Logs, Cameroon: 735 values converted to NaN\n",
            "WARNING:root:Column Logs, Malaysian: 735 values converted to NaN\n",
            "WARNING:root:Column Maize: 735 values converted to NaN\n",
            "WARNING:root:Column Metals  & Minerals: 735 values converted to NaN\n",
            "WARNING:root:Column Natural gas index: 735 values converted to NaN\n",
            "WARNING:root:Column Natural gas, Europe: 735 values converted to NaN\n",
            "WARNING:root:Column Natural gas, US: 735 values converted to NaN\n",
            "WARNING:root:Column Nickel: 735 values converted to NaN\n",
            "WARNING:root:Column Non-energy: 735 values converted to NaN\n",
            "WARNING:root:Column Oils & Meals: 735 values converted to NaN\n",
            "WARNING:root:Column Orange: 735 values converted to NaN\n",
            "WARNING:root:Column Other Food : 735 values converted to NaN\n",
            "WARNING:root:Column Other Raw Mat: 735 values converted to NaN\n",
            "WARNING:root:Column Palm kernel oil: 735 values converted to NaN\n",
            "WARNING:root:Column Palm oil: 735 values converted to NaN\n",
            "WARNING:root:Column Phosphate rock: 735 values converted to NaN\n",
            "WARNING:root:Column Platinum: 735 values converted to NaN\n",
            "WARNING:root:Column Plywood: 735 values converted to NaN\n",
            "WARNING:root:Column Potassium chloride **: 735 values converted to NaN\n",
            "WARNING:root:Column Precious Metals: 735 values converted to NaN\n",
            "WARNING:root:Column Rapeseed oil: 735 values converted to NaN\n",
            "WARNING:root:Column Raw Materials: 735 values converted to NaN\n",
            "WARNING:root:Column Rice, Thai 25%: 735 values converted to NaN\n",
            "WARNING:root:Column Rice, Thai 5%: 735 values converted to NaN\n",
            "WARNING:root:Column Rice, Thai A.1: 735 values converted to NaN\n",
            "WARNING:root:Column Rice, Viet Namese 5%: 735 values converted to NaN\n",
            "WARNING:root:Column Rubber, RSS3: 735 values converted to NaN\n",
            "WARNING:root:Column Rubber, TSR20 **: 735 values converted to NaN\n",
            "WARNING:root:Column Sawnwood, Cameroon: 735 values converted to NaN\n",
            "WARNING:root:Column Sawnwood, Malaysian: 735 values converted to NaN\n",
            "WARNING:root:Column Shrimps, Mexican: 735 values converted to NaN\n",
            "WARNING:root:Column Silver: 735 values converted to NaN\n",
            "WARNING:root:Column Sorghum: 735 values converted to NaN\n",
            "WARNING:root:Column Soybean meal: 735 values converted to NaN\n",
            "WARNING:root:Column Soybean oil: 735 values converted to NaN\n",
            "WARNING:root:Column Soybeans: 735 values converted to NaN\n",
            "WARNING:root:Column Sugar, EU: 735 values converted to NaN\n",
            "WARNING:root:Column Sugar, US: 735 values converted to NaN\n",
            "WARNING:root:Column Sugar, world: 735 values converted to NaN\n",
            "WARNING:root:Column Sunflower oil: 735 values converted to NaN\n",
            "WARNING:root:Column TSP: 735 values converted to NaN\n",
            "WARNING:root:Column Tea, Colombo: 735 values converted to NaN\n",
            "WARNING:root:Column Tea, Kolkata: 735 values converted to NaN\n",
            "WARNING:root:Column Tea, Mombasa: 735 values converted to NaN\n",
            "WARNING:root:Column Tea, avg 3 auctions: 735 values converted to NaN\n",
            "WARNING:root:Column Timber: 735 values converted to NaN\n",
            "WARNING:root:Column Tin: 735 values converted to NaN\n",
            "WARNING:root:Column Tobacco, US import u.v.: 735 values converted to NaN\n",
            "WARNING:root:Column Total Index: 735 values converted to NaN\n",
            "WARNING:root:Column Urea: 735 values converted to NaN\n",
            "WARNING:root:Column Wheat, US HRW: 735 values converted to NaN\n",
            "WARNING:root:Column Wheat, US SRW: 735 values converted to NaN\n",
            "WARNING:root:Column Zinc: 735 values converted to NaN\n",
            "<ipython-input-13-8b171748a444>:133: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df = df.fillna(method='ffill', limit=5)\n",
            "<ipython-input-13-8b171748a444>:134: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df = df.fillna(method='bfill', limit=5)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
            "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
            "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
            "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
            "  return xp.asarray(numpy.nanmax(X, axis=axis))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m69/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 2s/step - directional_accuracy: 0.0000e+00 - loss: nan - mae: nan - mse: nan"
          ]
        }
      ]
    }
  ]
}